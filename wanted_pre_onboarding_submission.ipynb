{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wanted_pre_onboarding_submission.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyO+HvIlTk63Hqd3gj3EIeiF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 수강생 선발과제 안내\n","\n","- 아래 설명에 따라 코드의 빈칸을 채워 Tokenizer(문제 1)와 TfidfVectorizer(문제 2) 클래스를 완성하세요.\n","    - 문제 1, 문제 2 모두 수행해야 합니다.\n","    - 주어진 조건을 모두 만족해야 합니다.\n","- 작업한 파일을 하나의 GitHub Repository에 담아서 제출하세요.\n","    - 파일 형식: `.ipynb` (문제 1과 문제 2 작업 결과를 모두 담아 주시기 바랍니다.) "],"metadata":{"id":"XLaVySZOTWN8"}},{"cell_type":"markdown","source":["## **문제 1) Tokenizer 생성하기**\n","\n","**1-1. `preprocessing()`**\n","\n","텍스트 전처리를 하는 함수입니다.\n","\n","- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n","- output: 각 문장을 토큰화한 결과로, nested list 형태입니다. ex) [['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n","- 조건 1: 입력된 문장에 대해서 소문자로의 변환과 특수문자 제거를 수행합니다.\n","- 조건 2: 토큰화는 white space 단위로 수행합니다.\n","    \n","    \n","\n","**1-2. `fit()`**\n","\n","어휘 사전을 구축하는 함수입니다.\n","\n","- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n","- 조건 1: 위에서 만든 `preprocessing` 함수를 이용하여 각 문장에 대해 토큰화를 수행합니다.\n","- 조건 2: 각각의 토큰을 정수 인덱싱 하기 위한 어휘 사전(`self.word_dict`)을 생성합니다.\n","    - 주어진 코드에 있는 `self.word_dict`를 활용합니다.\n","    \n","\n","**1-3. `transform()`**\n","\n","어휘 사전을 활용하여 입력 문장을 정수 인덱싱하는 함수입니다.\n","\n","- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n","- output: 각 문장의 정수 인덱싱으로, nested list 형태입니다. ex) [[1, 2, 3, 4], [1, 5, 6]]\n","- 조건 1: 어휘 사전(`self.word_dict`)에 없는 단어는 'oov'의 index로 변환합니다."],"metadata":{"id":"20E4LmuqTpyE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDtCmJFiTFZj"},"outputs":[],"source":["class Tokenizer():\n","  def __init__(self):\n","    self.word_dict = {'oov': 0}\n","    self.fit_checker = False\n","  \n","  def preprocessing(self, sequences):\n","    result = []\n","    '''\n","    문제 1-1.\n","    '''\n","    import re\n","    result = [re.sub( r\"[^a-z\\s]\", \"\", seq.lower()).split() for seq in sequences]\n","    return result\n","  \n","  def fit(self, sequences):\n","    self.fit_checker = False\n","    '''\n","    문제 1-2.\n","    '''\n","    i = 1\n","    for tokens in Tokenizer.preprocessing(self, sequences):\n","        for token in tokens:\n","            if token not in self.word_dict.keys():\n","                self.word_dict[token] = i\n","                i += 1\n","    \n","    self.fit_checker = True\n","  \n","  def transform(self, sequences):\n","    result = []\n","    tokens = self.preprocessing(sequences)\n","    if self.fit_checker:\n","      '''\n","      문제 1-3.\n","      '''\n","      result = []\n","      for seq in Tokenizer.preprocessing(self, sequences):\n","          result.append([self.word_dict[token] if token in self.word_dict.keys() else self.word_dict['oov'] for token in seq])\n","      return result\n","    else:\n","      raise Exception(\"Tokenizer instance is not fitted yet.\")\n","      \n","  def fit_transform(self, sequences):\n","    self.fit(sequences)\n","    result = self.transform(sequences)\n","    return result"]},{"cell_type":"markdown","source":["Test Case 1. ['I go to school.', 'I LIKE pizza!']"],"metadata":{"id":"DKQnQPTujXLX"}},{"cell_type":"code","source":["examples = ['I go to school.', 'I LIKE pizza!']\n","\n","tokenizer = Tokenizer()\n","print(f\"- Preprocessed results: {tokenizer.preprocessing(examples)}\")\n","\n","tokenizer.fit(examples)\n","print(\"\\n- Word Dictionary\")\n","print(tokenizer.word_dict)\n","\n","print(f\"\\n- Transformed results: {tokenizer.transform(examples)}\")"],"metadata":{"id":"CERz87d9WavN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test Case 2. ['I love U:)']  (Out of Vocab)"],"metadata":{"id":"ZixKpW1PjqXI"}},{"cell_type":"code","source":["# TEST\n","tokenizer.transform(['I love U:)'])"],"metadata":{"id":"qadXhQtNgeS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"9dvg02yXiZAz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **문제 2) TfidfVectorizer 생성하기**\n","\n","**2-1. `fit()`**\n","\n","입력 문장들을 이용해 IDF 행렬을 만드는 함수입니다.\n","\n","- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n","- 조건 1: IDF 행렬은 list 형태입니다.\n","    - ex) [토큰1에 대한 IDF 값, 토큰2에 대한 IDF 값, .... ]\n","- 조건 2: IDF 값은 아래 식을 이용해 구합니다.\n","  \n","  $$ \n","  idf(d,t)=log_e(\\frac{n}{1+df(d,t)}) \n","  $$\n","    \n","    - $df(d,t)$ : 단어 t가 포함된 문장 d의 개수\n","    - $n$ : 입력된 전체 문장 개수\n","- 조건 3: 입력된 문장의 토큰화에는 문제 1에서 만든 Tokenizer를 사용합니다.\n","    \n","\\\n","\n","**2-2. `transform()`**\n","\n","입력 문장들을 이용해 TF-IDF 행렬을 만드는 함수입니다.\n","\n","- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n","- output : nested list 형태입니다.\n","    \n","    ex) [[tf-idf(1, 1), tf-idf(1, 2), tf-idf(1, 3)], [tf-idf(2, 1), tf-idf(2, 2), tf-idf(2, 3)]]\n","\n","> |  | 토큰1 | 토큰2 | 토큰3 |\n","    | --- | --- | --- | --- |\n","    | 문장1 | tf-idf(1,1) | tf-idf(1,2) | tf-idf(1,3) |\n","    | 문장2 | tf-idf(2,1) | tf-idf(2,2) | tf-idf(2,3) |\n","\n","- 조건1 : 입력 문장을 이용해 TF 행렬을 만드세요.\n","    - $tf(d, t)$ : 문장 d에 단어 t가 나타난 횟수\n","- 조건2 : 문제 2-1( `fit()`)에서 만든 IDF 행렬과 아래 식을 이용해 TF-IDF 행렬을 만드세요\n","    \n","    $$\n","    tf-idf(d,t) = tf(d,t) \\times idf(d,t)\n","    $$"],"metadata":{"id":"YfUk6wZDkCTq"}},{"cell_type":"code","source":["class TfidfVectorizer:\n","    def __init__(self, tokenizer):\n","        self.tokenizer = tokenizer\n","        self.fit_checker = False\n","  \n","    def fit(self, sequences):\n","        tokenized = self.tokenizer.fit_transform(sequences)\n","        '''\n","        문제 2-1.\n","        '''\n","        import numpy as np\n","        \n","        df = [0] * len(self.tokenizer.word_dict)  # document frequency\n","        for index_seq in tokenized:  # tokenized = [[1,2,3,4], [1,5,6]]\n","            seen = [0] * len(self.tokenizer.word_dict)\n","            for index in index_seq:\n","                if not(seen[index]):\n","                    seen[index] = 1\n","                    df[index] += 1\n","    \n","        self.idf_matrix = np.round(np.log( len(sequences) / (1 + np.array(df)) ),2)\n","        self.fit_checker = True\n","    \n","    def transform(self, sequences):\n","        if self.fit_checker:\n","            tokenized = self.tokenizer.transform(sequences)\n","            '''\n","            문제 2-2.\n","            '''\n","            import numpy as np\n","            n, v = len(sequences), len(self.tokenizer.word_dict)\n","            self.tf_matrix = np.zeros((n,v))\n","            for d, index_seq in enumerate(tokenized):\n","                for index in index_seq:\n","                    self.tf_matrix[d, index] += 1\n","            print(self.tf_matrix)\n","            print(self.idf_matrix)\n","            \n","            self.tfidf_matrix = self.tf_matrix * self.idf_matrix\n","            return self.tfidf_matrix\n","        else:\n","            raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n","\n","  \n","    def fit_transform(self, sequences):\n","        self.fit(sequences)\n","        return self.transform(sequences)"],"metadata":{"id":"Zx0C_Ef9kD73"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["examples = ['I go to school.', 'I LIKE pizza!']\n","\n","tokenizer = Tokenizer()\n","tfidfvectorizer = TfidfVectorizer(tokenizer)\n","\n","# tfidfvectorizer.fit(examples)\n","tfidfvectorizer.fit_transform(examples)"],"metadata":{"id":"5lZ71TDitjsi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tfidfvectorizer = TfidfVectorizer(tokenizer)\n","tfidfvectorizer.fit(examples)\n","print(f\"Word Dictionary: {tokenizer.word_dict}\")\n","\n","examples_2 = ['I go to school. i i', 'I LIKE pizza!', 'Do you like me??', 'Yes, I really really/34']\n","tfidfvectorizer.transform(examples_2)"],"metadata":{"id":"Ir7olhzItyT0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","df = np.array([0,2,1,1,1,1,1])\n","np.log(2 / (1+df))"],"metadata":{"id":"l-eRCtF6zeqr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.log(4/3)"],"metadata":{"id":"iUt_z-lE1tNi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.log(2)"],"metadata":{"id":"zsR28NnKA7cO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"s4U9pk6wA-ue"},"execution_count":null,"outputs":[]}]}